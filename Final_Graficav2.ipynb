{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision ultralytics albumentations scikit-learn opencv-python pillow matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3x8uN6_XXKds",
        "outputId": "3dbd599a-b165-4845-ee3f-9128d196fb59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.162)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.8)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.14)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.11.7)\n",
            "Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.0.24)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations) (4.11.0.86)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (3.12.5)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (6.4.9)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"tapakah68/supervisely-filtered-segmentation-person-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTwy990uYVnc",
        "outputId": "312bd8d6-6a23-45d3-bc84-00c1977032a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/supervisely-filtered-segmentation-person-dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Copia todo el dataset a /content/fashionpedia\n",
        "#shutil.copytree(path, '/content/fashionpedia', dirs_exist_ok=True)\n",
        "shutil.copytree(path, '/content/persons', dirs_exist_ok=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DbEFwyYwctMG",
        "outputId": "a5948a15-ed7e-4544-e203-a5b7a67190ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/persons'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sistema de Segmentación de Personas y Extracción de Colores con Transfer Learning**\n",
        "\n",
        "Este proyecto implementa un sistema completo que:\n",
        "- Segmenta personas en imágenes usando YOLOv8 con máscaras.\n",
        "- Extrae colores dominantes del torso y piernas de las personas detectadas.\n",
        "- Aplica Transfer Learning para adaptar un modelo preentrenado a un nuevo dataset.\n",
        "- Genera visualizaciones y resultados en formato JSON.\n",
        "\n",
        "### **Importación de Bibliotecas**\n",
        "\n",
        "En esta sección importamos todas las dependencias necesarias para el proyecto:\n",
        "\n",
        "- **PyTorch**: Framework principal para el manejo de redes neuronales\n",
        "- **OpenCV (cv2)**: Procesamiento de imágenes y visión por computadora\n",
        "- **YOLO (Ultralytics)**: Implementación del modelo de segmentación\n",
        "- **Scikit-learn**: Para algoritmos de clustering (K-means)\n",
        "- **Otras utilidades**: Manipulación de archivos, visualización, etc.\n",
        "\n",
        "Estas bibliotecas nos permitirán desde cargar imágenes hasta entrenar modelos de deep learning."
      ],
      "metadata": {
        "id": "G3_UkSCeogno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.cluster import KMeans\n",
        "from collections import Counter\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "import json\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLQSHpG8otZt",
        "outputId": "6a2accad-84d3-41da-90bd-4cbe2771cc04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Configuración de Paleta de Colores**\n",
        "\n",
        "Definimos una paleta de 18 colores básicos en español que usaremos para clasificar las prendas de vestir. Cada color tiene:\n",
        "\n",
        "- Nombre en español (ej. \"azul_marino\")\n",
        "- Valores RGB de referencia\n",
        "- Se usarán para etiquetar automáticamente los colores detectados en las prendas\n",
        "\n",
        "Esta configuración es especialmente útil para aplicaciones de moda y retail donde los nombres de colores deben ser comprensibles para usuarios finales."
      ],
      "metadata": {
        "id": "NsTZ0pT0o3Qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "COLORS = [\n",
        "   'negro', 'blanco', 'gris', 'azul', 'azul_marino', 'azul_claro',\n",
        "   'rojo', 'rosa', 'verde', 'verde_oscuro', 'amarillo', 'naranja',\n",
        "   'marron', 'beige', 'violeta', 'morado', 'dorado', 'plateado'\n",
        "]"
      ],
      "metadata": {
        "id": "dWoRR8FCo8qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dataset Supervisely para Segmentación**\n",
        "\n",
        "La clase `SuperviselyDataset` maneja el conjunto de datos [Human Segmentation Dataset](https://supervise.ly/) que contiene:\n",
        "\n",
        "- **Imágenes**: Fotografías de personas con diversas vestimentas\n",
        "- **Máscaras**: Anotaciones casi pixel-perfect de las siluetas humanas\n",
        "\n",
        "Funcionalidades:\n",
        "- Carga automática de pares imagen/máscara\n",
        "- Filtrado de imágenes corruptas o sin anotaciones\n",
        "- Compatible con el formato YOLOv8 para segmentación\n"
      ],
      "metadata": {
        "id": "seG08w-cpB94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SuperviselyDataset(Dataset):\n",
        "    \"\"\"Dataset para entrenar YOLO con el dataset Human Segmentation Dataset - Supervise.ly\"\"\"\n",
        "    def __init__(self, images_dir, masks_dir, img_size=640):\n",
        "        self.images_dir = Path(images_dir)\n",
        "        self.masks_dir = Path(masks_dir)\n",
        "        self.img_size = img_size\n",
        "\n",
        "        # Obtener lista de imágenes válidas\n",
        "        self.valid_samples = []\n",
        "        for img_path in self.images_dir.glob('*.png'):\n",
        "            # Buscar máscara correspondiente (puede ser .png o .jpg)\n",
        "            mask_path_png = self.masks_dir / f\"{img_path.stem}.png\"\n",
        "            mask_path_jpg = self.masks_dir / f\"{img_path.stem}.jpg\"\n",
        "\n",
        "            if mask_path_png.exists():\n",
        "                self.valid_samples.append((img_path, mask_path_png))\n",
        "            elif mask_path_jpg.exists():\n",
        "                self.valid_samples.append((img_path, mask_path_jpg))\n",
        "\n",
        "        print(f\"Encontradas {len(self.valid_samples)} imágenes válidas con máscaras\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, mask_path = self.valid_samples[idx]\n",
        "        return str(img_path), str(mask_path)"
      ],
      "metadata": {
        "id": "TM4JItvgpASz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Extracción de Colores Dominantes**\n",
        "\n",
        "El `ColorExtractor` realiza:\n",
        "\n",
        "1. **Clustering con K-means**: Identifica los colores predominantes en una región\n",
        "2. **Filtrado**:\n",
        "   - Elimina píxeles demasiado oscuros/claros (sombras/reflejos)\n",
        "   - Manejo de casos especiales (regiones vacías)\n",
        "3. **Mapeo a nombres**:\n",
        "   - Convierte valores RGB a nombres comprensibles\n",
        "   - Usa distancias euclidianas para encontrar el color más cercano\n",
        "\n",
        "Ejemplo de uso:\n",
        "```python\n",
        "extractor = ColorExtractor()\n",
        "color_name, rgb = extractor.extract_dominant_color(region_imagen)"
      ],
      "metadata": {
        "id": "DpPlTBEFpI24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ColorExtractor:\n",
        "    \"\"\"Extractor de colores dominantes mejorado\"\"\"\n",
        "    def __init__(self):\n",
        "        self.color_names = COLORS\n",
        "\n",
        "    def extract_dominant_color(self, image_region, n_colors=3):\n",
        "        \"\"\"Extrae el color dominante de una región\"\"\"\n",
        "        if image_region.size == 0:\n",
        "            return 'gris', [128, 128, 128]\n",
        "\n",
        "        # Reshape para K-means\n",
        "        pixels = image_region.reshape(-1, 3)\n",
        "\n",
        "        # Remover píxeles muy oscuros o muy claros (posibles sombras/reflejos)\n",
        "        pixels = pixels[np.all(pixels > 20, axis=1)]  # Remover muy oscuros\n",
        "        pixels = pixels[np.all(pixels < 235, axis=1)]  # Remover muy claros\n",
        "\n",
        "        if len(pixels) < 10:\n",
        "            return 'gris', [128, 128, 128]\n",
        "\n",
        "        # Aplicar K-means\n",
        "        kmeans = KMeans(n_clusters=min(n_colors, len(pixels)), random_state=42, n_init=10)\n",
        "        kmeans.fit(pixels)\n",
        "\n",
        "        # Obtener colores y sus frecuencias\n",
        "        colors = kmeans.cluster_centers_\n",
        "        labels = kmeans.labels_\n",
        "\n",
        "        # Contar frecuencias\n",
        "        label_counts = Counter(labels)\n",
        "\n",
        "        # Obtener el color más frecuente\n",
        "        dominant_idx = label_counts.most_common(1)[0][0]\n",
        "        dominant_color_rgb = colors[dominant_idx].astype(int)\n",
        "\n",
        "        # Convertir RGB a nombre de color\n",
        "        color_name = self.rgb_to_color_name(dominant_color_rgb)\n",
        "\n",
        "        return color_name, dominant_color_rgb\n",
        "\n",
        "    def rgb_to_color_name(self, rgb):\n",
        "        \"\"\"Convierte RGB a nombre de color más preciso\"\"\"\n",
        "        color_map = {\n",
        "            'negro': [0, 0, 0],\n",
        "            'blanco': [255, 255, 255],\n",
        "            'gris': [128, 128, 128],\n",
        "            'azul': [0, 0, 255],\n",
        "            'azul_marino': [0, 0, 128],\n",
        "            'azul_claro': [173, 216, 230],\n",
        "            'rojo': [255, 0, 0],\n",
        "            'rosa': [255, 192, 203],\n",
        "            'verde': [0, 255, 0],\n",
        "            'verde_oscuro': [0, 100, 0],\n",
        "            'amarillo': [255, 255, 0],\n",
        "            'naranja': [255, 165, 0],\n",
        "            'marron': [165, 42, 42],\n",
        "            'beige': [245, 245, 220],\n",
        "            'violeta': [238, 130, 238],\n",
        "            'morado': [128, 0, 128],\n",
        "            'dorado': [255, 215, 0],\n",
        "            'plateado': [192, 192, 192]\n",
        "        }\n",
        "\n",
        "        min_distance = float('inf')\n",
        "        closest_color_name = 'gris'\n",
        "\n",
        "        for color_name, color_rgb in color_map.items():\n",
        "            distance = np.sqrt(sum((c1 - c2) ** 2 for c1, c2 in zip(rgb, color_rgb)))\n",
        "            if distance < min_distance:\n",
        "                min_distance = distance\n",
        "                closest_color_name = color_name\n",
        "\n",
        "        return closest_color_name\n"
      ],
      "metadata": {
        "id": "P0rqb-hCpJaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Clase PersonSegmentationSystem**\n",
        "### **Sistema de Segmentación de Personas con Transfer Learning**\n",
        "\n",
        "Clase central que implementa un pipeline completo para:\n",
        "1. Segmentación precisa de personas usando YOLOv8\n",
        "2. Análisis de color de prendas de vestir\n",
        "3. Generación de reportes visuales y JSON\n",
        "\n",
        "**Arquitectura basada en Transfer Learning**:\n",
        "- Utiliza YOLOv8-seg como modelo base (pre-entrenado en COCO)\n",
        "- Fine-tuning especializado en segmentación de personas\n",
        "- Capas convolucionales congeladas en primeras etapas\n",
        "- Head de segmentación ajustable\n",
        "\n",
        "### **Métodos de Procesamiento**\n",
        "1. `prepare_supervisely_dataset()`:\n",
        "   - Convierte un dataset Supervise.ly al formato YOLOv8-seg\n",
        "   - Extrae contornos de máscaras binarizadas\n",
        "   - Genera etiquetas .txt con coordenadas normalizadas\n",
        "   - Organiza carpetas de imágenes y etiquetas (train/val)\n",
        "   - Crea archivo YAML con configuración del dataset\n",
        "\n",
        "2. `train_yolo_person_segmentation()`:\n",
        "   - Realiza el fine-tuning del modelo YOLOv8\n",
        "   - Ajusta pesos del modelo base al nuevo dominio\n",
        "   - Imprime métricas por época si están disponibles\n",
        "\n",
        "3. `load_trained_model()`:\n",
        "   - Carga de un modelo YOLOv8-seg previamente entrenado\n",
        "   - Permite realizar inferencia sin repetir el entrenamiento\n",
        "\n",
        "4. `segment_and_extract_colors()`:\n",
        "   - Segmentación con confianza ajustable\n",
        "   - División inteligente en regiones (torso/piernas)\n",
        "   - Extracción de color dominante por región\n",
        "\n",
        "5. `visualize_segmentation_results()`:\n",
        "   - Visualización comparativa\n",
        "   - Overlay de colores semitransparente\n",
        "   - Exportación a alta resolución\n",
        "\n",
        "6. `batch_process_images()`:\n",
        "   - Procesamiento paralelizado\n",
        "   - Generación de reportes JSON\n",
        "   - Resumen estadístico\n"
      ],
      "metadata": {
        "id": "tT9yWndipRaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PersonSegmentationSystem:\n",
        "    \"\"\"Sistema completo: Segmentación de personas + extracción de colores\"\"\"\n",
        "\n",
        "    def __init__(self, base_model='yolov8n-seg.pt'):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Usando dispositivo: {self.device}\")\n",
        "\n",
        "        # Modelo YOLO de segmentación\n",
        "        self.yolo_model = YOLO(base_model)\n",
        "        self.color_extractor = ColorExtractor()\n",
        "\n",
        "        #print(yolo_model.names) para verificar las clases si se desea\n",
        "\n",
        "        # Configuración para crear dataset YOLO\n",
        "        self.yolo_config = {\n",
        "            'train': '',\n",
        "            'val': '',\n",
        "            'nc': 1,  # Solo clase persona\n",
        "            'names': {0: 'person'}\n",
        "        }\n",
        "\n",
        "    def prepare_supervisely_dataset(self, images_dir, masks_dir, output_dir):\n",
        "        \"\"\"Convierte dataset Supervise.ly para entrenar YOLO en segmentación de personas\"\"\"\n",
        "\n",
        "        # Crear directorios de salida\n",
        "        output_path = Path(output_dir)\n",
        "        (output_path / 'images' / 'train').mkdir(parents=True, exist_ok=True)\n",
        "        (output_path / 'images' / 'val').mkdir(parents=True, exist_ok=True)\n",
        "        (output_path / 'labels' / 'train').mkdir(parents=True, exist_ok=True)\n",
        "        (output_path / 'labels' / 'val').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Procesar imágenes\n",
        "        images_dir = Path(images_dir)\n",
        "        masks_dir = Path(masks_dir)\n",
        "\n",
        "        processed_count = 0\n",
        "\n",
        "        for img_path in images_dir.glob('*.png'):\n",
        "            # Buscar máscara correspondiente\n",
        "            mask_path_png = masks_dir / f\"{img_path.stem}.png\"\n",
        "            mask_path_jpg = masks_dir / f\"{img_path.stem}.jpg\"\n",
        "\n",
        "            mask_path = None\n",
        "            if mask_path_png.exists():\n",
        "                mask_path = mask_path_png\n",
        "            elif mask_path_jpg.exists():\n",
        "                mask_path = mask_path_jpg\n",
        "\n",
        "            if mask_path is None:\n",
        "                continue\n",
        "\n",
        "            # Cargar imagen y máscara\n",
        "            image = cv2.imread(str(img_path))\n",
        "            mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "            if image is None or mask is None:\n",
        "                continue\n",
        "\n",
        "            h, w = image.shape[:2]\n",
        "\n",
        "            # Binarizar máscara\n",
        "            _, mask_binary = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "            # Encontrar contornos de personas\n",
        "            contours, _ = cv2.findContours(mask_binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "            if len(contours) == 0:\n",
        "                continue\n",
        "\n",
        "            # Determinar si va a train o val (80-20 split)\n",
        "            subset = 'train' if processed_count % 5 != 0 else 'val'\n",
        "\n",
        "            # Copiar imagen\n",
        "            dst_img = output_path / 'images' / subset / img_path.name\n",
        "            cv2.imwrite(str(dst_img), image)\n",
        "\n",
        "            # Crear etiquetas YOLO para segmentación\n",
        "            dst_label = output_path / 'labels' / subset / f\"{img_path.stem}.txt\"\n",
        "\n",
        "            with open(dst_label, 'w') as f:\n",
        "                for contour in contours:\n",
        "                    # Obtener bbox del contorno\n",
        "                    x, y, w_bbox, h_bbox = cv2.boundingRect(contour)\n",
        "\n",
        "                    # Filtrar contornos muy pequeños\n",
        "                    if w_bbox < 20 or h_bbox < 20:\n",
        "                        continue\n",
        "\n",
        "                    # Convertir contorno a coordenadas normalizadas\n",
        "                    contour_normalized = []\n",
        "                    for point in contour:\n",
        "                        x_norm = point[0][0] / w\n",
        "                        y_norm = point[0][1] / h\n",
        "                        contour_normalized.extend([x_norm, y_norm])\n",
        "\n",
        "                    # Simplificar contorno si es muy complejo\n",
        "                    if len(contour_normalized) > 200:  # Máximo 100 puntos\n",
        "                        epsilon = 0.01 * cv2.arcLength(contour, True)\n",
        "                        contour_simplified = cv2.approxPolyDP(contour, epsilon, True)\n",
        "                        contour_normalized = []\n",
        "                        for point in contour_simplified:\n",
        "                            x_norm = point[0][0] / w\n",
        "                            y_norm = point[0][1] / h\n",
        "                            contour_normalized.extend([x_norm, y_norm])\n",
        "\n",
        "                    # Escribir en formato YOLO segmentación\n",
        "                    if len(contour_normalized) >= 6:  # Mínimo 3 puntos\n",
        "                        line = \"0 \" + \" \".join(map(str, contour_normalized))\n",
        "                        f.write(line + \"\\n\")\n",
        "\n",
        "            processed_count += 1\n",
        "\n",
        "            if processed_count % 100 == 0:\n",
        "                print(f\"Procesadas {processed_count} imágenes...\")\n",
        "\n",
        "        # Crear archivo de configuración YOLO\n",
        "        self.yolo_config['train'] = str(output_path / 'images' / 'train')\n",
        "        self.yolo_config['val'] = str(output_path / 'images' / 'val')\n",
        "\n",
        "        config_path = output_path / 'person_segmentation.yaml'\n",
        "        with open(config_path, 'w') as f:\n",
        "            yaml.dump(self.yolo_config, f)\n",
        "\n",
        "        print(f\"Dataset preparado: {processed_count} imágenes procesadas\")\n",
        "        print(f\"Configuración guardada en: {config_path}\")\n",
        "\n",
        "        return str(config_path)\n",
        "\n",
        "    def train_yolo_person_segmentation(self, dataset_config_path, epochs=100, batch_size=16):\n",
        "        \"\"\"Entrena YOLO para segmentación de personas\"\"\"\n",
        "\n",
        "        print(\"Iniciando entrenamiento YOLO para segmentación de personas...\")\n",
        "\n",
        "        # Entrenar modelo\n",
        "        results = self.yolo_model.train(\n",
        "            data=dataset_config_path,\n",
        "            epochs=epochs,\n",
        "            batch=batch_size,\n",
        "            imgsz=640,\n",
        "            device=self.device,\n",
        "            workers=4,\n",
        "            patience=10,\n",
        "            save=True,\n",
        "            project='person_segmentation',\n",
        "            name='yolo_person_seg_v1'\n",
        "        )\n",
        "\n",
        "        print(\"Entrenamiento completado!\")\n",
        "\n",
        "        # Mostrar loss y métricas por época\n",
        "        results_csv = \"person_segmentation/yolo_person_seg_v1/results.csv\"\n",
        "        if os.path.exists(results_csv):\n",
        "            df = pd.read_csv(results_csv)\n",
        "            print(\"\\nResumen de entrenamiento (loss y métricas por época):\")\n",
        "            print(df[[\"epoch\", \"train/box_loss\", \"train/seg_loss\", \"metrics/mAP_0.5\"]])\n",
        "        else:\n",
        "            print(\"No se encontró el archivo de resultados para mostrar métricas.\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def load_trained_model(self, model_path):\n",
        "        \"\"\"Cargar modelo YOLO entrenado\"\"\"\n",
        "        self.yolo_model = YOLO(model_path)\n",
        "        print(f\"Modelo cargado: {model_path}\")\n",
        "\n",
        "    def segment_and_extract_colors(self, image_path, conf_threshold=0.5):\n",
        "        \"\"\"Segmenta personas y extrae colores de torso y piernas usando las máscaras\"\"\"\n",
        "\n",
        "        # Realizar segmentación\n",
        "        results = self.yolo_model(image_path, conf=conf_threshold)\n",
        "\n",
        "        # Cargar imagen original\n",
        "        image = cv2.imread(image_path)\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        h, w = image_rgb.shape[:2]\n",
        "\n",
        "        detections = []\n",
        "\n",
        "        for result in results:\n",
        "            if result.masks is not None:\n",
        "                boxes = result.boxes\n",
        "                masks = result.masks\n",
        "\n",
        "                for i in range(len(boxes)):\n",
        "                    # Obtener bbox y confianza\n",
        "                    x1, y1, x2, y2 = boxes.xyxy[i].cpu().numpy()\n",
        "                    confidence = boxes.conf[i].cpu().numpy()\n",
        "\n",
        "                    # Obtener máscara\n",
        "                    mask = masks.data[i].cpu().numpy()\n",
        "\n",
        "                    # Redimensionar máscara al tamaño original\n",
        "                    mask_resized = cv2.resize(mask, (w, h), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "                    # Crear máscara binaria\n",
        "                    mask_binary = (mask_resized > 0.5).astype(np.uint8)\n",
        "\n",
        "                    # Aplicar máscara a la imagen\n",
        "                    person_pixels = image_rgb[mask_binary == 1]\n",
        "\n",
        "                    if len(person_pixels) > 0:\n",
        "                        # Dividir la máscara en regiones de torso y piernas\n",
        "                        mask_coords = np.where(mask_binary == 1)\n",
        "                        y_coords = mask_coords[0]\n",
        "\n",
        "                        # Calcular límites para torso y piernas\n",
        "                        y_min, y_max = np.min(y_coords), np.max(y_coords)\n",
        "                        person_height = y_max - y_min\n",
        "\n",
        "                        # Torso: 60% superior\n",
        "                        torso_limit = y_min + int(person_height * 0.6)\n",
        "\n",
        "                        # Crear máscaras para torso y piernas\n",
        "                        torso_mask = mask_binary.copy()\n",
        "                        torso_mask[torso_limit:, :] = 0\n",
        "\n",
        "                        legs_mask = mask_binary.copy()\n",
        "                        legs_mask[:torso_limit, :] = 0\n",
        "\n",
        "                        # Extraer píxeles de torso y piernas\n",
        "                        torso_pixels = image_rgb[torso_mask == 1]\n",
        "                        legs_pixels = image_rgb[legs_mask == 1]\n",
        "\n",
        "                        # Extraer colores dominantes\n",
        "                        torso_color, torso_rgb = self.color_extractor.extract_dominant_color(torso_pixels)\n",
        "                        legs_color, legs_rgb = self.color_extractor.extract_dominant_color(legs_pixels)\n",
        "\n",
        "                        detections.append({\n",
        "                            'bbox': [int(x1), int(y1), int(x2-x1), int(y2-y1)],\n",
        "                            'confidence': float(confidence),\n",
        "                            'mask': mask_binary,\n",
        "                            'torso_color': torso_color,\n",
        "                            'torso_rgb': torso_rgb.tolist(),\n",
        "                            'legs_color': legs_color,\n",
        "                            'legs_rgb': legs_rgb.tolist()\n",
        "                        })\n",
        "\n",
        "        return detections\n",
        "\n",
        "    def visualize_segmentation_results(self, image_path, detections, save_path=None):\n",
        "        \"\"\"Visualiza resultados de segmentación y colores\"\"\"\n",
        "\n",
        "        image = cv2.imread(image_path)\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
        "\n",
        "        # Imagen original con bboxes\n",
        "        axes[0].imshow(image_rgb)\n",
        "        axes[0].set_title('Detección Original', fontsize=14)\n",
        "\n",
        "        # Imagen con máscaras\n",
        "        axes[1].imshow(image_rgb)\n",
        "        axes[1].set_title('Segmentación con Colores', fontsize=14)\n",
        "\n",
        "        colors_overlay = np.zeros_like(image_rgb)\n",
        "\n",
        "        for i, detection in enumerate(detections):\n",
        "            bbox = detection['bbox']\n",
        "            x, y, w, h = bbox\n",
        "            mask = detection['mask']\n",
        "\n",
        "            # Dibujar bbox en imagen original\n",
        "            rect = plt.Rectangle((x, y), w, h, fill=False, color='red', linewidth=2)\n",
        "            axes[0].add_patch(rect)\n",
        "\n",
        "            # Crear overlay de colores para la segmentación\n",
        "            torso_rgb = np.array(detection['torso_rgb'])\n",
        "            legs_rgb = np.array(detection['legs_rgb'])\n",
        "\n",
        "            # Dividir máscara en torso y piernas\n",
        "            mask_coords = np.where(mask == 1)\n",
        "            y_coords = mask_coords[0]\n",
        "\n",
        "            if len(y_coords) > 0:\n",
        "                y_min, y_max = np.min(y_coords), np.max(y_coords)\n",
        "                person_height = y_max - y_min\n",
        "                torso_limit = y_min + int(person_height * 0.6)\n",
        "\n",
        "                # Colorear torso\n",
        "                torso_region = mask.copy()\n",
        "                torso_region[torso_limit:, :] = 0\n",
        "                colors_overlay[torso_region == 1] = torso_rgb * 0.6\n",
        "\n",
        "                # Colorear piernas\n",
        "                legs_region = mask.copy()\n",
        "                legs_region[:torso_limit, :] = 0\n",
        "                colors_overlay[legs_region == 1] = legs_rgb * 0.6\n",
        "\n",
        "            # Información de colores\n",
        "            confidence = detection['confidence']\n",
        "            torso_color = detection['torso_color']\n",
        "            legs_color = detection['legs_color']\n",
        "\n",
        "            text = f\"Persona {i+1}\\nTorso: {torso_color}\\nPiernas: {legs_color}\\nConf: {confidence:.2f}\"\n",
        "\n",
        "            axes[0].text(x, y-60, text, fontsize=10, color='white', weight='bold',\n",
        "                        bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"red\", alpha=0.8))\n",
        "\n",
        "        # Superponer colores\n",
        "        axes[1].imshow(colors_overlay, alpha=0.7)\n",
        "\n",
        "        for ax in axes:\n",
        "            ax.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
        "            print(f\"Resultado guardado: {save_path}\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    def batch_process_images(self, images_dir, output_dir):\n",
        "        \"\"\"Procesa múltiples imágenes y guarda resultados\"\"\"\n",
        "\n",
        "        images_dir = Path(images_dir)\n",
        "        output_dir = Path(output_dir)\n",
        "        output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        results_summary = []\n",
        "\n",
        "        for img_path in images_dir.glob('*.png'):\n",
        "            print(f\"Procesando: {img_path.name}\")\n",
        "\n",
        "            # Segmentar y extraer colores\n",
        "            detections = self.segment_and_extract_colors(str(img_path))\n",
        "\n",
        "            # Guardar visualización\n",
        "            save_path = output_dir / f\"segmentation_result_{img_path.name}\"\n",
        "            self.visualize_segmentation_results(str(img_path), detections, str(save_path))\n",
        "\n",
        "            # Preparar datos para JSON (sin la máscara binaria)\n",
        "            json_detections = []\n",
        "            for det in detections:\n",
        "                json_det = det.copy()\n",
        "                json_det.pop('mask', None)  # Remover máscara para JSON\n",
        "                json_detections.append(json_det)\n",
        "\n",
        "            # Guardar datos en JSON\n",
        "            result_data = {\n",
        "                'image': img_path.name,\n",
        "                'detections': json_detections,\n",
        "                'total_persons': len(detections)\n",
        "            }\n",
        "\n",
        "            results_summary.append(result_data)\n",
        "\n",
        "            # Mostrar resumen\n",
        "            print(f\"  Detectadas {len(detections)} personas\")\n",
        "            for i, det in enumerate(detections):\n",
        "                print(f\"    Persona {i+1}: Torso={det['torso_color']}, \"\n",
        "                      f\"Piernas={det['legs_color']}, Conf={det['confidence']:.2f}\")\n",
        "\n",
        "        # Guardar resumen\n",
        "        with open(output_dir / 'segmentation_results_summary.json', 'w') as f:\n",
        "            json.dump(results_summary, f, indent=2)\n",
        "\n",
        "        print(f\"\\nProcesamiento completado. Resultados en: {output_dir}\")\n",
        "        return results_summary"
      ],
      "metadata": {
        "id": "99-k08mRr1Lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Demo: Flujo de Trabajo Completo**\n",
        "\n",
        "La función `main()` muestra un ejemplo completo con 3 pasos:\n",
        "\n",
        "1. **Preparación de datos**:\n",
        "   - Conversión del formato Supervisely a YOLO\n",
        "   - División train/val (80/20)\n",
        "\n",
        "2. **Entrenamiento (Transfer Learning):**\n",
        "   - Inicio del fine-tuning del modelo YOLOv8-seg\n",
        "   - Reutiliza pesos preentrenados (COCO) y los ajusta al nuevo dataset\n",
        "   - Monitoreo de métricas por época (pérdida, mAP)\n",
        "\n",
        "3. **Inferencia**:\n",
        "   - Procesamiento de imágenes nuevas\n",
        "   - Generación de reportes visuales"
      ],
      "metadata": {
        "id": "ASAmac_fr4We"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Función principal de demostración\"\"\"\n",
        "\n",
        "    # Configurar rutas para el dataset Human Segmentation Dataset - Supervise.ly\n",
        "    SUPERVISELY_ROOT = \"/content/persons/supervisely_person_clean_2667_img/supervisely_person_clean_2667_img\"\n",
        "    SUPERVISELY_IMAGES = f\"{SUPERVISELY_ROOT}/images\"\n",
        "    SUPERVISELY_MASKS = f\"{SUPERVISELY_ROOT}/masks\"\n",
        "\n",
        "    PERSON_DATASET_DIR = \"/content/persons_segmented\"\n",
        "    TEST_IMAGES_DIR = \"/content/test_images\"\n",
        "    OUTPUT_DIR = \"/content/results\"\n",
        "\n",
        "    # Crear sistema\n",
        "    system = PersonSegmentationSystem()\n",
        "\n",
        "    # Paso 1: Preparar dataset de segmentación desde Supervise.ly\n",
        "    print(\"=== PASO 1: Preparando dataset de segmentación ===\")\n",
        "    dataset_config = system.prepare_supervisely_dataset(\n",
        "        SUPERVISELY_IMAGES,\n",
        "        SUPERVISELY_MASKS,\n",
        "        PERSON_DATASET_DIR\n",
        "    )\n",
        "\n",
        "    # Paso 2: Entrenar YOLO para segmentación de personas\n",
        "    print(\"\\n=== PASO 2: Entrenando YOLO para segmentación ===\")\n",
        "    system.train_yolo_person_segmentation(\n",
        "        dataset_config,\n",
        "        epochs=10,\n",
        "        batch_size=8\n",
        "    )\n",
        "\n",
        "    #print(\"\\n=== PROCESO COMPLETADO ===\")\n",
        "    #print(f\"Resultados guardados en: {OUTPUT_DIR}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "wP9PMFT3r6Qi",
        "outputId": "2e251928-f450-4db4-9009-461a99af0ff0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando dispositivo: cuda\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n-seg.pt to 'yolov8n-seg.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6.74M/6.74M [00:00<00:00, 153MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== PASO 1: Preparando dataset de segmentación ===\n",
            "Procesadas 100 imágenes...\n",
            "Procesadas 200 imágenes...\n",
            "Procesadas 300 imágenes...\n",
            "Procesadas 400 imágenes...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-15-2798686658.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-15-2798686658.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Paso 1: Preparar dataset de segmentación desde Supervise.ly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=== PASO 1: Preparando dataset de segmentación ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     dataset_config = system.prepare_supervisely_dataset(\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mSUPERVISELY_IMAGES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mSUPERVISELY_MASKS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-14-3378333086.py\u001b[0m in \u001b[0;36mprepare_supervisely_dataset\u001b[0;34m(self, images_dir, masks_dir, output_dir)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;31m# Cargar imagen y máscara\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_GRAYSCALE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "   # Paso 3: Cargar modelo entrenado\n",
        "   print(\"\\n=== PASO 3: Cargando modelo entrenado ===\")\n",
        "   model_path = \"person_segmentation/yolo_person_seg_v1/weights/best.pt\"\n",
        "   system.load_trained_model(model_path)\n",
        "\n",
        "   # Paso 4: Procesar imágenes de test\n",
        "   print(\"\\n=== PASO 4: Procesando imágenes de test ===\")\n",
        "   results = system.batch_process_images(TEST_IMAGES_DIR, OUTPUT_DIR)\n",
        "\n",
        "   print(\"\\n=== PROCESO COMPLETADO ===\")\n",
        "   print(f\"Resultados guardados en: {OUTPUT_DIR}\")"
      ],
      "metadata": {
        "id": "2QUccDMVXOw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajusta la ruta al modelo entrenado\n",
        "model_path = \"/content/person_segmentation/yolo_person_seg_v13/weights/best.pt\"\n",
        "\n",
        "# Define las rutas si no están ya definidas\n",
        "TEST_IMAGES_DIR = \"/content/test_images\"  # Asegúrate de que esta ruta sea correcta\n",
        "OUTPUT_DIR = \"/content/results\"      # Asegúrate de que esta ruta sea correcta\n",
        "\n",
        "# Crear una instancia del sistema si no existe (esto puede ser redundante si se llama después de main)\n",
        "# Si ya tienes la instancia 'system' creada y las rutas definidas:\n",
        "# system.load_trained_model(model_path)\n",
        "\n",
        "# Crear sistema\n",
        "system = PersonSegmentationSystem()\n",
        "\n",
        "system.load_trained_model(model_path)\n",
        "\n",
        "# Procesa las imágenes de test y guarda los resultados\n",
        "results = system.batch_process_images(TEST_IMAGES_DIR, OUTPUT_DIR)\n",
        "\n",
        "print(\"\\n=== PROCESO COMPLETADO ===\")\n",
        "print(f\"Resultados guardados en: {OUTPUT_DIR}\")"
      ],
      "metadata": {
        "id": "3V0HphkB9FiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OLD VERSION**"
      ],
      "metadata": {
        "id": "li7hbAhT9DpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Rutas (ajusta si es necesario)\n",
        "MODEL_PATH = \"person_detection/yolo_person_v1/weights/best.pt\"\n",
        "TEST_IMAGES_DIR = \"/content/test_images\"\n",
        "OUTPUT_DIR = \"/content/results\"\n",
        "\n",
        "# Crear sistema y cargar modelo entrenado\n",
        "system = PersonColorDetectionSystem()\n",
        "system.load_trained_model(MODEL_PATH)\n",
        "\n",
        "# Procesar imágenes de test (no reentrena nada)\n",
        "results = system.batch_process_images(TEST_IMAGES_DIR, OUTPUT_DIR)\n",
        "\n",
        "print(\"Resultados guardados en:\", OUTPUT_DIR)"
      ],
      "metadata": {
        "id": "1nbSFEOBD0OY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "id": "BZEGAxKhFyJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HRI4obgbInu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!du -sh person_detection\n"
      ],
      "metadata": {
        "id": "mjulwWyLIYP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r test_images/"
      ],
      "metadata": {
        "id": "0ePAFsa99kcT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}